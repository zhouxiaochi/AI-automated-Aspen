#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¡¨æ ¼è§£æå·¥å…·ï¼šå°†åˆ†å‰²åçš„tableæ–‡ä»¶è§£æä¸ºCSVæ ¼å¼
æ£€æµ‹å¹¶å¤„ç†æç«¯æƒ…å†µï¼ˆå¦‚æ•°æ®ä¸æˆå¯¹çš„æƒ…å†µï¼‰
"""

from pathlib import Path
import csv
import re

def parse_table_content(lines):
    """è§£æè¡¨æ ¼å†…å®¹ï¼Œè¿”å›(alias, name)å¯¹åˆ—è¡¨"""
    aliases_names = []
    
    # è·³è¿‡è¡¨å¤´ï¼Œæ‰¾åˆ° P11 P10 (å®Œæ•´æˆ–ç®€åŒ–æ ¼å¼) ä¹‹åçš„å†…å®¹
    start_idx = -1
    for i, line in enumerate(lines):
        line_stripped = line.strip()
        # æ”¯æŒä¸¤ç§æ ¼å¼: "P11 P10 P93 P856PCD" æˆ– "P11 P10"
        if line_stripped == "P11 P10" or line_stripped == "P11 P10 P93 P856PCD":
            start_idx = i + 2  # è·³è¿‡è¡¨å¤´è¡Œå’Œä¸‹ä¸€ä¸ª "---"
            break
    
    if start_idx == -1:
        return aliases_names, "æœªæ‰¾åˆ° P11 P10 è¡¨å¤´"
    
    # æå–æ•°æ®è¡Œï¼ˆæ’é™¤ "---" å’Œ "X"ï¼‰ï¼Œé‡åˆ°é¡µé¢æ ‡è®°æ—¶åœæ­¢
    data_lines = []
    stop_keywords = ["Pure Component", "Databanks", "===== PAGE", "Physical Property Data", "Available in Databank"]
    
    for i in range(start_idx, len(lines)):
        line = lines[i].strip()
        
        # æ£€æŸ¥æ˜¯å¦é‡åˆ°åœæ­¢å…³é”®è¯
        if any(keyword in line for keyword in stop_keywords):
            break
            
        if line and line != "---" and line != "X":
            data_lines.append(line)
    
    # æ£€æŸ¥æ•°æ®è¡Œæ•°æ˜¯å¦ä¸ºå¶æ•°
    if len(data_lines) % 2 != 0:
        return aliases_names, f"æ•°æ®è¡Œæ•°ä¸ºå¥‡æ•° ({len(data_lines)} è¡Œ)ï¼Œåº”è¯¥æ˜¯å¶æ•°ï¼ˆalias-nameé…å¯¹ï¼‰"
    
    # æŒ‰ç…§ alias, name é…å¯¹å¤„ç†
    for i in range(0, len(data_lines), 2):
        alias = data_lines[i].strip().upper()
        name = data_lines[i + 1].strip().upper()
        
        # åŸºæœ¬éªŒè¯
        if alias and name:
            aliases_names.append((alias, name))
    
    return aliases_names, None

def detect_extreme_cases(table_file):
    """æ£€æµ‹æç«¯æƒ…å†µçš„è¡¨æ ¼"""
    
    # å·²çŸ¥çš„æç«¯æƒ…å†µè¡¨æ ¼
    extreme_cases = {
        "table_72.txt": "GLYCæ•°æ®æ ¼å¼å¼‚å¸¸"
    }
    
    return table_file.name in extreme_cases, extreme_cases.get(table_file.name, "")

def process_single_table(table_file, output_dir):
    """å¤„ç†å•ä¸ªè¡¨æ ¼æ–‡ä»¶"""
    
    print(f"ğŸ“‹ å¤„ç†è¡¨æ ¼: {table_file.name}")
    
    # æ£€æµ‹æç«¯æƒ…å†µ
    is_extreme, extreme_reason = detect_extreme_cases(table_file)
    if is_extreme:
        print(f"âš ï¸  è·³è¿‡æç«¯æƒ…å†µ: {extreme_reason}")
        return False, 0, extreme_reason
    
    # è¯»å–æ–‡ä»¶
    try:
        with open(table_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except Exception as e:
        error_msg = f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}"
        print(f"âŒ {error_msg}")
        return False, 0, error_msg
    
    # è§£æå†…å®¹
    aliases_names, error = parse_table_content(lines)
    
    if error:
        print(f"âŒ è§£æé”™è¯¯: {error}")
        return False, 0, error
    
    if not aliases_names:
        print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ alias-name é…å¯¹")
        return False, 0, "æ²¡æœ‰æœ‰æ•ˆæ•°æ®"
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    csv_file = output_dir / f"{table_file.stem}.csv"
    
    try:
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['databank', 'alias_or_code', 'registered_name'])
            
            for alias, name in aliases_names:
                writer.writerow(['PURE11', alias, name])
        
        print(f"âœ… æˆåŠŸè¾“å‡º: {len(aliases_names)} å¯¹ â†’ {csv_file.name}")
        return True, len(aliases_names), None
        
    except Exception as e:
        error_msg = f"å†™å…¥CSVå¤±è´¥: {e}"
        print(f"âŒ {error_msg}")
        return False, 0, error_msg

def process_all_tables(tables_dir, output_dir):
    """å¤„ç†æ‰€æœ‰è¡¨æ ¼æ–‡ä»¶"""
    
    print("ğŸ”§ åŒ–åˆç‰©è¡¨æ ¼è§£æå·¥å…·")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(exist_ok=True)
    
    # è·å–æ‰€æœ‰è¡¨æ ¼æ–‡ä»¶
    table_files = sorted(tables_dir.glob("table_*.txt"))
    
    if not table_files:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°è¡¨æ ¼æ–‡ä»¶åœ¨ {tables_dir}")
        return
    
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {tables_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“„ æ‰¾åˆ° {len(table_files)} ä¸ªè¡¨æ ¼æ–‡ä»¶")
    print()
    
    # ç»Ÿè®¡å˜é‡
    success_count = 0
    total_pairs = 0
    failed_tables = []
    extreme_tables = []
    
    # é€ä¸ªå¤„ç†
    for table_file in table_files:
        success, pairs, error = process_single_table(table_file, output_dir)
        
        if success:
            success_count += 1
            total_pairs += pairs
        else:
            failed_tables.append((table_file.name, error))
            if "æç«¯æƒ…å†µ" in str(error):
                extreme_tables.append(table_file.name)
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print()
    print("ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡")
    print("=" * 60)
    print(f"âœ… æˆåŠŸå¤„ç†: {success_count} ä¸ªè¡¨æ ¼")
    print(f"ğŸ“ æå–é…å¯¹: {total_pairs:,} å¯¹ alias-name")
    print(f"âŒ å¤±è´¥è¡¨æ ¼: {len(failed_tables)} ä¸ª")
    print(f"âš ï¸  æç«¯æƒ…å†µ: {len(extreme_tables)} ä¸ª")
    
    # æ˜¾ç¤ºå¤±è´¥è¯¦æƒ…
    if failed_tables:
        print(f"\nâŒ å¤±è´¥è¡¨æ ¼è¯¦æƒ…:")
        for table_name, error in failed_tables:
            print(f"   {table_name}: {error}")
    
    # æ˜¾ç¤ºæç«¯æƒ…å†µ
    if extreme_tables:
        print(f"\nâš ï¸  æç«¯æƒ…å†µè¡¨æ ¼:")
        for table_name in extreme_tables:
            print(f"   {table_name}")
    
    print(f"\nğŸ¯ æˆåŠŸç‡: {success_count}/{len(table_files)} ({success_count/len(table_files)*100:.1f}%)")

def main():
    # è¾“å…¥å’Œè¾“å‡ºè·¯å¾„
    tables_dir = Path("example/data/tables")
    output_dir = Path("example/data/parsed_csv")
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not tables_dir.exists():
        print(f"âŒ è¡¨æ ¼ç›®å½•ä¸å­˜åœ¨: {tables_dir}")
        print("è¯·å…ˆè¿è¡Œ split_tables.py æ¥åˆ†å‰²è¡¨æ ¼")
        return
    
    # å¤„ç†æ‰€æœ‰è¡¨æ ¼
    process_all_tables(tables_dir, output_dir)

if __name__ == "__main__":
    main() 